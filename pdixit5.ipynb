{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pratibha Dixith (pdixit5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import NaiveBayes\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.evaluation  import BinaryClassificationMetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#csv.reader(open(\"train.csv\",'r',encoding='ISO-8859-1'))\n",
    "#sc = SparkContext()\n",
    "\n",
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "tweetsentilist = []\n",
    "featurelist = []\n",
    "wordlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cleantweets(eachtweet):\n",
    "    stemmer=PorterStemmer() \n",
    "    #for eachtweet in onlytweets:\n",
    "    #print(eachtweet)\n",
    "    if 'http' in eachtweet:\n",
    "        #print(eachtweet)\n",
    "        eachtweet=re.sub('https?://[^\\s]+','URL',eachtweet)\n",
    "    if 'www.' in eachtweet:\n",
    "        eachtweet=re.sub('www\\.[^\\s]+','URL',eachtweet)\n",
    "    if '@' in eachtweet:\n",
    "        eachtweet=re.sub('@[^\\s]+','AT_USER',eachtweet)\n",
    "    #eachtweet=re.findall(r\"[\\w']+\", eachtweet)\n",
    "    tweetwordlist=eachtweet.strip().split(' ')\n",
    "    eachtweet=''\n",
    "    for eachword in tweetwordlist:\n",
    "        #print(eachword)\n",
    "        #print(len(eachword))\n",
    "        eachword=re.sub('[^\\w]+','',eachword)\n",
    "        pattern = re.compile(r'(.)\\1{1,}', re.DOTALL) \n",
    "        eachword =  pattern.sub(r'\\1\\1', eachword)\n",
    "        #print(eachword)\n",
    "        eachword=stemmer.stem(eachword) \n",
    "        #print(eachword)\n",
    "            \n",
    "        if eachword.startswith('#'):\n",
    "            eachword=eachword.lstrip('#')\n",
    "        if  len(eachword)!=0 :\n",
    "            if not eachword[0].isalpha():\n",
    "                #print(eachword)\n",
    "                eachword=''\n",
    "        if eachword not in stopwords:\n",
    "            if eachtweet=='':\n",
    "                eachtweet=eachword\n",
    "            elif eachword=='':\n",
    "                eachtweet=eachtweet\n",
    "            else:\n",
    "                eachtweet=eachtweet+' '+eachword\n",
    "    return eachtweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perdictiontruepositive(predictiontweet):\n",
    "    if predictiontweet[0] == 1 and predictiontweet[1]==1:\n",
    "        return predictiontweet\n",
    "    \n",
    "def perdictiontruenegative(predictiontweet):\n",
    "    if predictiontweet[0] == 0 and predictiontweet[1]==0:\n",
    "        return predictiontweet\n",
    "\n",
    "def perdictionfalsepositive(predictiontweet):\n",
    "    if predictiontweet[0] == 1 and predictiontweet[1]==0:\n",
    "        return predictiontweet\n",
    "    \n",
    "def perdictionfalsenegative(predictiontweet):\n",
    "    if predictiontweet[0] == 0 and predictiontweet[1]==1:\n",
    "        return predictiontweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metrics(predictiontweet):\n",
    "\n",
    "    predicttruepositive = predictiontweet.filter(perdictiontruepositive)\n",
    "    predictflasepositive = predictiontweet.filter(perdictionfalsepositive)\n",
    "    print(\"predicttruepositive:\" + str(predicttruepositive.count()))\n",
    "    print(\"predictflasepositive:\"+str(predictflasepositive.count()))\n",
    "    precision = predicttruepositive.count()/(predicttruepositive.count()+predictflasepositive.count())\n",
    "    print(\"precision : \"+str(precision))\n",
    "\n",
    "    predictflasenegative = predictiontweet.filter(perdictionfalsenegative)\n",
    "    predicttruenegative = predictiontweet.filter(perdictiontruenegative)\n",
    "    print(\"predictflasenegative:\" + str(predictflasenegative.count()))\n",
    "    print(\"predicttruenegative:\"+str(predicttruenegative.count()))\n",
    "    recall = predicttruepositive.count()/(predicttruepositive.count()+predictflasenegative.count())\n",
    "\n",
    "    print(\"recall : \"+str(recall))\n",
    "\n",
    "    fmeasure = 2*(precision*recall)/(recall+precision)\n",
    "    \n",
    "    perfmetrics = BinaryClassificationMetrics(predictiontweet)\n",
    "\n",
    "    print(\"fmeasure: \"+str(fmeasure))\n",
    "    print(\"area under ROC: \"+str(perfmetrics.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot(naivedata,lrdata,dtdata):\n",
    "\n",
    "    #y = [81.3175, 87.36, 62.2187]\n",
    "    #x = [74.72, 71.825, 62.12]\n",
    "    #z = [79.38, 77.99, 60.16]\n",
    "    N = len(naivedata)\n",
    "#x = range(N)\n",
    "    ind = np.arange(N)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.ylim(0,100)\n",
    "    ax = plt.subplot(111)\n",
    "    #width = 1/1.5\n",
    "    rects1 = ax.bar(ind-0.2, naivedata, width=0.2, color=\"blue\")\n",
    "    rects2 = ax.bar(ind, lrdata, width=0.2, color=\"red\")\n",
    "    rects3 = ax.bar(ind+.2, dtdata, width=0.2, color=\"green\")\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Accuracy comparison between all classification')\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels(('Training Accuracy', 'K-fold mean Accuracy', 'Test Accuracy'))\n",
    "    ax.legend((rects1[0], rects2[0],rects3[0]), ('Naive-Baiye', 'Logistic Regression','Decision Tree'))\n",
    "\n",
    "\n",
    "#fig = plt.gcf()\n",
    "#plot_url = py.plot_mpl(fig, filename='mpl-basic-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive-Bayes classifier train-train accuracy: 81.3175\n"
     ]
    }
   ],
   "source": [
    "def featurevector(row):\n",
    "    row=row.split('\\\",')\n",
    "    tweet = cleantweets(row[5].lower())\n",
    "    return (row[0].lstrip('\"'),tweet.split())\n",
    "\n",
    "def tweettext(row):\n",
    "    line=row.split('\\\",')\n",
    "    return (line[5])\n",
    "            \n",
    "\n",
    "tweetsenti=sc.textFile('train.csv').map(featurevector)    \n",
    "tweetsenti.persist()\n",
    "tweetobj = sc.textFile('test.csv').map(tweettext)\n",
    "testsenti=sc.textFile('test.csv').map(featurevector)   \n",
    "testsenti.persist()\n",
    "#print(tweetsentilist)\n",
    "#for x in tweetsenti.collect():\n",
    " #   print(x)\n",
    "    \n",
    "naiveplot = []\n",
    "lrplot = []\n",
    "dtplot = []\n",
    "    \n",
    "htf = HashingTF(50000)\n",
    "\n",
    "tweettrainhash=tweetsenti.map(lambda senti:LabeledPoint(float(senti[0]),htf.transform(senti[1])))\n",
    "testtrainhash = testsenti.map(lambda senti:LabeledPoint(float(senti[0]),htf.transform(senti[1])))\n",
    "train_hashed=tweettrainhash.randomSplit([0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1])\n",
    "#train_hashed,test_hashed=tweettrainhash.randomSplit([0.7,0.3])\n",
    "\n",
    "\n",
    "naiveBayesmodel = NaiveBayes.train(tweettrainhash)\n",
    "predictiontweet = tweettrainhash.map(lambda pointvalue: (float(naiveBayesmodel.predict(pointvalue.features)),pointvalue.label))\n",
    "accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(tweettrainhash.count())\n",
    "naiveplot.append(accuracy*100)\n",
    "print(\"Naive-Bayes classifier train-train accuracy: \"+str(accuracy*100) )\n",
    "\n",
    "\n",
    "mean=0.0\n",
    "for i in range(0,10):\n",
    "    traindata= sc.emptyRDD()\n",
    "    testdata=train_hashed[i]\n",
    "    for cnt in range(0,10):\n",
    "        if cnt!=i:\n",
    "            traindata=traindata+train_hashed[cnt]\n",
    "    naiveBayesmodel = NaiveBayes.train(traindata)\n",
    "    #logisticregressionmodel = LogisticRegressionWithLBFGS.train(traindata)\n",
    "    #decisiontreenmodel = DecisionTree.trainClassifier(traindata, numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=5, maxBins=32)\n",
    "    #predictions = decisiontreenmodel.predict(testdata.map(lambda x: x.features))\n",
    "    #predictiontweet = testdata.map(lambda lp:lp.label).zip(predictions)\n",
    "    predictiontweet = testdata.map(lambda pointvalue: (float(naiveBayesmodel.predict(pointvalue.features)),pointvalue.label))\n",
    "    #print(predictiontweet.take(100))\n",
    "    accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(testdata.count())\n",
    "    print(\"Naive-Bayes classifier K-fold accuracy: \"+str(accuracy*100) )\n",
    "    #cnt=cnt+1\n",
    "    mean=mean+accuracy\n",
    "naiveplot.append((mean/10)*100)\n",
    "print(\"Naive-Bayes classifier average for K-fold classifier: \"+str((mean/10)*100) )  \n",
    "\n",
    "naiveBayesmodel = NaiveBayes.train(tweettrainhash)\n",
    "predictiontweet = testtrainhash.map(lambda pointvalue: (float(naiveBayesmodel.predict(pointvalue.features)),pointvalue.label))\n",
    "accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(testtrainhash.count())\n",
    "naiveplot.append(accuracy*100)\n",
    "print(\"Naive-Bayes classifier train-test accuracy: \"+str(accuracy*100) )\n",
    "\n",
    "print(\"Performance metrics for Naive-Bayes\")\n",
    "metrics(predictiontweet)\n",
    "\n",
    "logisticregressionmodel = LogisticRegressionWithLBFGS.train(tweettrainhash)\n",
    "predictiontweet = tweettrainhash.map(lambda pointvalue: (float(logisticregressionmodel.predict(pointvalue.features)),pointvalue.label))\n",
    "accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(tweettrainhash.count())\n",
    "lrplot.append(accuracy*100)\n",
    "print(\"Logistic Regression train-train classifier: \"+str(accuracy*100) )\n",
    "\n",
    "mean=0.0\n",
    "for i in range(0,10):\n",
    "    traindata= sc.emptyRDD()\n",
    "    testdata=train_hashed[i]\n",
    "    for cnt in range(0,10):\n",
    "        if cnt!=i:\n",
    "            traindata=traindata+train_hashed[cnt]\n",
    "    logisticregressionmodel = LogisticRegressionWithLBFGS.train(traindata)\n",
    "    predictiontweet = testdata.map(lambda pointvalue: (float(logisticregressionmodel.predict(pointvalue.features)),pointvalue.label))\n",
    "    accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(testdata.count())\n",
    "    print(\"Logistic Regression classifier K-fold accuracy: \"+str(accuracy*100) )\n",
    "    mean=mean+accuracy\n",
    "lrplot.append((mean/10)*100)\n",
    "print(\"Logistic Regression classifier average for K-fold classifier: \"+str((mean/10)*100) )    \n",
    "\n",
    "\n",
    "logisticregressionmodel = LogisticRegressionWithLBFGS.train(tweettrainhash)\n",
    "predictiontweet = testtrainhash.map(lambda pointvalue: (float(logisticregressionmodel.predict(pointvalue.features)),pointvalue.label))\n",
    "accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(testtrainhash.count())\n",
    "lrplot.append(accuracy*100)\n",
    "print(\"Logistic Regression train-test classifier: \"+str(accuracy*100) )\n",
    "\n",
    "print(\"Performance metrics for Logistic Regression\")\n",
    "metrics(predictiontweet)\n",
    "\n",
    "#train_hashed=tweettrainhash.randomSplit([0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1])\n",
    "#mean=0.0\n",
    "  \n",
    "    \n",
    "#print(tweetsentilist)        \n",
    "#print (onlytweets)\n",
    "\n",
    "htfdectree = HashingTF(5000)\n",
    "dectweettrainhash=tweetsenti.map(lambda senti:LabeledPoint(senti[0],htfdectree.transform(senti[1])))\n",
    "dectesttrainhash = testsenti.map(lambda senti:LabeledPoint(senti[0],htfdectree.transform(senti[1])))\n",
    "train_hashed=dectweettrainhash.randomSplit([0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1])\n",
    "\n",
    "\n",
    "decisiontreenmodel = DecisionTree.trainClassifier(dectweettrainhash, numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=5, maxBins=32)\n",
    "predictions = decisiontreenmodel.predict(dectweettrainhash.map(lambda x: x.features))\n",
    "predictiontweet = dectweettrainhash.map(lambda lp:lp.label).zip(predictions)\n",
    "accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(dectweettrainhash.count())\n",
    "dtplot.append(accuracy*100)\n",
    "print(\"Decision tree train-train classifier: \"+str(accuracy*100) )\n",
    "\n",
    "mean=0.0\n",
    "for i in range(0,10):\n",
    "    traindata= sc.emptyRDD()\n",
    "    testdata=train_hashed[i]\n",
    "    for cnt in range(0,10):\n",
    "        if cnt!=i:\n",
    "            traindata=traindata+train_hashed[cnt]\n",
    "    decisiontreenmodel = DecisionTree.trainClassifier(traindata, numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=5, maxBins=32)\n",
    "    predictions = decisiontreenmodel.predict(testdata.map(lambda x: x.features))\n",
    "    predictiontweet = testdata.map(lambda lp:lp.label).zip(predictions)\n",
    "    accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(testdata.count())\n",
    "    print(\"Decision tree classifier K-fold accuracy: \"+str(accuracy*100) )\n",
    "    mean=mean+accuracy\n",
    "dtplot.append(mean*100)\n",
    "print(\"Decision tree Regression classifier anerage for K-fold classifier: \"+str((mean/10)*100))\n",
    "\n",
    "\n",
    "#dtplot.append(62)\n",
    "#dtplot.append(62)\n",
    "decisiontreenmodel = DecisionTree.trainClassifier(dectweettrainhash, numClasses=2, categoricalFeaturesInfo={},impurity='gini', maxDepth=5, maxBins=32)\n",
    "predictions = decisiontreenmodel.predict(dectesttrainhash.map(lambda x: x.features))\n",
    "predictiontweet = dectesttrainhash.map(lambda lp:lp.label).zip(predictions)\n",
    "accuracy = predictiontweet.filter(lambda prediction:prediction[0]==prediction[1]).count()/float(dectesttrainhash.count())\n",
    "dtplot.append(accuracy*100)\n",
    "print(\"Decision tree train-test classifier: \"+str(accuracy*100) )\n",
    "\n",
    "print(\"Performance metrics for Decision tree\")\n",
    "metrics(predictiontweet)\n",
    "\n",
    "##train_hashed,test_hashed=tweettrainhash.randomSplit([0.7,0.3])\n",
    "\n",
    "plot(naiveplot,lrplot,dtplot)\n",
    "\n",
    "logisticregressionmodel.clearThreshold()\n",
    "predictiontweet = testtrainhash.map(lambda pointvalue: (float(logisticregressionmodel.predict(pointvalue.features)),pointvalue.label))\n",
    "result = predictiontweet.zip(tweetobj)\n",
    "correcttweet = result.filter(lambda x : (x[0][0] >=0.5 and x[0][1] == 1) or (x[0][0] < 0.5 and x[0][1] == 0) )\n",
    "somecorrect = correcttweet.takeOrdered(20 , key = lambda p: p[0][0] )\n",
    "print()\n",
    "print(\"some 20 correctly classified tweet:\")\n",
    "for t in somecorrect:\n",
    "    print(t[0][0],t[1])\n",
    "\n",
    "print()\n",
    "print(\"Top 5 correctly classified tweet:\")\n",
    "top5 = correcttweet.takeOrdered(5 , key = lambda p: -p[0][0] )\n",
    "for t in top5:\n",
    "    print(t[0][0],t[1])\n",
    "\n",
    "\n",
    "correcttweet = result.filter(lambda x : (x[0][0] >=0.5 and x[0][1] == 0) or (x[0][0] < 0.5 and x[0][1] == 1) )\n",
    "somecorrect = correcttweet.takeOrdered(20 , key = lambda p: p[0][0] )\n",
    "print()\n",
    "print(\"Some 20 incorrectly classified tweet:\")\n",
    "for t in somecorrect:\n",
    "    print(t[0][0],t[1])\n",
    "    \n",
    "print()\n",
    "print(\"Top 5 incorrectly classified tweet:\")\n",
    "top5 = correcttweet.takeOrdered(5 , key = lambda p: -p[0][0] )\n",
    "for t in top5:\n",
    "    print(t[0][0],t[1])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def perdictiontruepositiveprobrate(predictiontweet,thres):\n",
    "    if predictiontweet[0] >= thres and predictiontweet[1]==1:\n",
    "        return predictiontweet\n",
    "    \n",
    "def perdictiontruenegativeprobrate(predictiontweet,thres):\n",
    "    if predictiontweet[0] < thres and predictiontweet[1]==0:\n",
    "        return predictiontweet\n",
    "\n",
    "def perdictionfalsepositiveprobrate(predictiontweet,thres):\n",
    "    if predictiontweet[0] >= thres and predictiontweet[1]==0:\n",
    "        return predictiontweet\n",
    "    \n",
    "def perdictionfalsenegativeprobrate(predictiontweet,thres):\n",
    "    if predictiontweet[0] < thres and predictiontweet[1]==1:\n",
    "        return predictiontweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probmetrics(predictiontweet,thres):\n",
    "\n",
    "    predicttruepositiveprob = predictiontweet.filter(lambda x:perdictiontruepositiveprobrate(x,thres))\n",
    "    predictflasepositiveprob = predictiontweet.filter(lambda x:perdictionfalsepositiveprobrate(x,thres))\n",
    "    predictflasenegative = predictiontweet.filter(lambda x:perdictionfalsenegativeprobrate(x,thres))\n",
    "    predicttruenegative = predictiontweet.filter(lambda x:perdictiontruenegativeprobrate(x,thres))\n",
    "\n",
    "    tpr = predicttruepositiveprob.count()/(predicttruepositiveprob.count()+predictflasenegative.count())\n",
    "    fpr = predictflasepositiveprob.count()/(predictflasepositiveprob.count()+predicttruenegative.count())\n",
    "    \n",
    "    return (tpr,fpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholdvalues = [0.0,0.25,0.5,0.75,1]\n",
    "xlist = []\n",
    "ylist = []\n",
    "\n",
    "for i in range(len(thresholdvalues)):\n",
    "    thres = thresholdvalues[i]\n",
    "    listVal  = probmetrics(predictiontweet,thres)\n",
    "    xlist.append(listVal[1])\n",
    "    ylist.append(listVal[0])\n",
    "plt.plot(xlist, ylist)\n",
    "plt.plot([0.0,1],[0.0,1],'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.title('Logistic Regression ROC')\n",
    "plt.legend(('Logistic Regression','Line Of No Discrimination'),loc=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
